**Made by Nicolò Baroncini (1834907)**

## Research (5_R)
### Explain a unified conceptual framework to obtain all most common measures of central tendency using the concept of distance (or “premetric” in general). Discuss why it is useful to discuss these concepts introducing the notion of distance.
The measures of central tendency are part of so called Summary Statistics. These measures identify a single value as representative of an entire distribution. This single value is chosen considering the concept of distance or, more generally, premetric.

For example, the arithmetic mean, which is a measure of central tendency, chooses as summary value the one that minimizes the sum of squared deviations from all the values of a dataset relatively to a variable, in symbols: d(xᵢ, x̄)². In this case the distance is defined as follow:

```
d(xᵢ, x̄) = xᵢ-x̄
```

where x̄ is the arithmetic mean. 

So to obtain a measure of central tendency, first of all, it's needed to define what we mean with distance, so we need to say what is the so-called distance function. After that, we need to decide what property we want to satisfy with the summary value relatively to the distance.

Let's now see this conceptual procedure to define the median. \
It is the value separating the higher half from the lower half of the values of a variable.\
In the sequence 
```
1, 3, 3, 6, 7, 8, 9 
```
the median is 6. To obtain it the sequence of the values must be ordered.\
This value can be defined considering the distance as the one that minimizes the sum of d(xᵢ, M) where M is the median and the distance is defined, equally to the arithmetic mean, as:


```
d(xᵢ, M) = xᵢ-M
```

It's useful to introduce these measures using distances because the measures of central tendency summarize a distribution considering the location of the values of a dataset or of a variable. The location of points can be defined as a distance from a point in the space. In these measures, this point is the one that satisfies some properties we decided. \
In this way it's possible to define an infinite amount of measures of central tendency just by modifying the definition of distance and the properties to satisfy, this allows to use a summary value that fits the most our purpose.

### Finally, point out the difference between the mathematical definition of "distance" and the properties of the "premetrics" useful in statistics, pointing out the most important distances, indexes and similarity measures used in statistics, data analysis and machine learning (such as for instance; Mahalanobis distance, Euclidean distance, Minkowski distance, Manhattan distance, Hamming distance, Cosine distance, Chebishev distance, Jaccard index, Haversine distance, Sørensen-Dice index, etc.).
**point out the difference between the mathematical definition of "distance" and the properties of the "premetrics" useful in statistics**

Let's now see some of the most used distances.

**Euclidean distance**

![1_hUrrJWqXysnlF4zMbNrKVg](https://user-images.githubusercontent.com/78324346/137699440-6289f5e2-0d8e-4a89-a436-6346f6396885.png)

This is the most common distance. This distance is the length of a segment conneting two points, its formula is

![image](https://user-images.githubusercontent.com/78324346/137699647-473968a7-a5cd-4e47-ba76-eec45b7dd4b1.png)

where x = (x₁,x₂,x₃,..,xₙ) and y = (y₁,y₂,y₃,...,yₙ). \
This distance has two major disadvantages:

***1-*** It is not scale invariant, it means that the distances depends on the units of the variables. \
***2-*** Higher-dimensional space does not act as we would, intuitively, expect from 2- or 3-dimensional space, so this makes harder to use effectively this distance for more than 3-dimensional spaces.

Although many other measures have been developed to account for the disadvantages of Euclidean distance, it is still one of the most used distance measures for good reasons. It is incredibly intuitive to use, simple to implement and shows great results in many use-cases.

**Cosine similarity**

![image](https://user-images.githubusercontent.com/78324346/137701352-ddd35ed6-9782-4ad9-9c51-225d5ddf1814.png)

Cosine similarity has often been used as a way to counteract Euclidean distance’s problem with high dimensionality. The cosine similarity is simply the cosine of the angle between two vectors.
Two vectors with exactly the same orientation have a cosine similarity of 1, whereas two vectors diametrically opposed to each other have a similarity of -1. In this distance the magnitude has not importance but only the orientation.

The formula is 

![image](https://user-images.githubusercontent.com/78324346/137701660-f90cf742-985a-4d51-9c9f-98f1de79704b.png)

where x = (x₁,x₂,x₃,..,xₙ) and y = (y₁,y₂,y₃,...,yₙ). \

The main disadvantage is that the magnitude of vectors is not taken into account, only their direction. So this distance is used when the magnitude in not important.

**Manhattan distance**

![image](https://user-images.githubusercontent.com/78324346/137701920-e2b316e3-1c3c-4c87-aa58-9b71c036202e.png)

It can compute the distance between two vectors if they could only move right angles, so there is no diagonal movement involved in calculating the distance. \
In formula is

![image](https://user-images.githubusercontent.com/78324346/137702304-f2ad1e47-bfa7-4109-af98-5da343c77196.png)

where x = (x₁,x₂,x₃,..,xₙ) and y = (y₁,y₂,y₃,...,yₙ). \

This distance is less intuitive than euclidean distance and it is more likely to give a higher distance value than euclidean distance since it does not the shortest path possible.

**Hamming distance**

![image](https://user-images.githubusercontent.com/78324346/137702769-39da1267-574e-4f82-a55d-e4577c8740ca.png)

Hamming distance is the number of values that are different between two vectors. It is typically used to compare two binary strings of equal length.

This kind of distance does not take the actual value into account as long as they are different or equal, this means that it can't be used when the magnitude is important.

**Chebyshev distance**

![image](https://user-images.githubusercontent.com/78324346/137702978-6034d88e-f2f0-41bd-b318-5a9b924b9cb3.png)

Chebyshev distance is defined as the greatest of difference between two vectors along any coordinate dimension.

The formula is

![image](https://user-images.githubusercontent.com/78324346/137703066-138fefde-9358-4ccc-a28e-99bac144ebaf.png)

where x = (x₁,x₂,x₃,..,xₙ) and y = (y₁,y₂,y₃,...,yₙ). \

Chebyshev is typically used in very specific use-cases, which makes it difficult to use as an all-purpose distance metric, like Euclidean distance or Cosine similarity.

**Minkowski distance**

![image](https://user-images.githubusercontent.com/78324346/137703614-70f7dd47-a027-46d8-9b0e-0e8ddf85b151.png)

This distance can be use in a normed vector space of any dimensional space, which means that it can be used in a space where distances can be represented as a vector that has a length.

This measure has three properties:
***1- Zero Vector:*** The zero vector has a length of zero whereas every other vector has a positive length. For example, if we travel from one place to another, then that distance is always positive. However, if we travel from one place to itself, then that distance is zero.

***2- Scalar Factor:*** When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.

***3- Triangle Inequality:*** The shortest distance between two points is a straight line.

The formula is:

![image](https://user-images.githubusercontent.com/78324346/137704316-6143dc33-f3f0-4a8a-8e1a-461aa8febb11.png)

Where we have that common values of p are:

• p=1 — Manhattan distance \
• p=2 — Euclidean distance \
• p=∞ — Chebyshev distance 

***Jaccard index***

![image](https://user-images.githubusercontent.com/78324346/137704657-aee69d76-9cac-4172-bb44-d9e9468a4728.png)

***Sørensen-Dice index***

![image](https://user-images.githubusercontent.com/78324346/137704750-9224631a-adeb-4a12-ac91-4dd38ff3904a.png)



*References:* \
[1] [https://en.wikipedia.org/wiki/Central_tendency](https://en.wikipedia.org/wiki/Central_tendency) \
[2] [https://en.wikipedia.org/wiki/Arithmetic_mean#Motivating_properties](https://en.wikipedia.org/wiki/Arithmetic_mean#Motivating_properties) \
[3] [https://en.wikipedia.org/wiki/Median](https://en.wikipedia.org/wiki/Median) \
[4] [https://math.stackexchange.com/questions/1199757/does-the-arithmetic-mean-minimize-the-sum-of-absolute-values-of-deviations](https://math.stackexchange.com/questions/1199757/does-the-arithmetic-mean-minimize-the-sum-of-absolute-values-of-deviations) \
[5] [https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa)
