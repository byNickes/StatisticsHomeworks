**Made by Nicolò Baroncini (1834907)**

## Research (7_R)
### Explain the Bayes Theorem and its key role in statistical induction.
In the research 4_R we have seen the Bayes theorem explained with relative frequencies. Since relative frequencies are the realization of the theoretical concept of probability, the Bayes theorem works also for probability. \
It says that the conditional probability P(A | B) can be calculated with the following formula:

![image](https://user-images.githubusercontent.com/78324346/138822456-dd78013c-dffc-463d-890a-372d1f861e38.png)

In statistical induction we have an empirical distribution, that has been obtained by observing a phenomenon, and a family of probability distribution. An example of family can be the family of normal distributions. Every member of the family can be obtained by changing, for instance, variance or mean. The parameters that can be set to pick a specific member of the family are contained in a vector called ***state of nature*** and its symbol is Θ. \
If we call E the empirical distribution we have that, in statistical induction, we are interested in knowing P(Θ | E). It is the probability that, knowing E, it was generated by a member of the familty distribution that has as values of the parameters the ones contained in Θ. \
The Bayes theorem comes handy to calculate this probability since:

![Foglio di brutta-6](https://user-images.githubusercontent.com/78324346/138826607-5c2497f8-cac3-4abf-9ee1-2ff3fc659c0b.jpg)

where\
•P(Θ) is the prior probability, it is the probability that Θ is the state of nature that generated E before that E is observed. \
•P(Θ|E) is the posterior probability. As already explained it is the probability that Θ generated E knowing E. \
•P(E|Θ) is called "likelihood of Θ", it is the probability that E is generated by Θ knowing Θ.\
•P(E) is the sum of the joint probability of E with any possible Θ since E can be generated by any of them.

This concept can be easily seen in the next example.

Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?

In this example the Θ is the bowl from which Fred picked a cookie and the E is the cookie picked. We want to know the probability P(Θ|E) which is the probability that the cookie, and so the empirical distribution, has been "generated" by the distribution represented by Θ that is the distribution of a specific bowl. \
Let Θ₁ correspond to bowl 1 and Θ₂ to bowl 2. It is given that the bowls are identical from Fred's point of view, thus P(Θ₁) = P(Θ₂) so both are equal to 0.5. The E is the observation that we picked a plain cookie. \
We know that P(E|Θ₁) = 30/40 = 0.75 and P(E|Θ₂) = 20/40 = 0.5.
So I get, using the Bayes theorem, the following value.

![Foglio di brutta-6 2](https://user-images.githubusercontent.com/78324346/138833383-b5e2c8a7-bbd5-4471-ab04-a71df449c50f.jpg)

Where:
• P(Θ₁) is the probability that Fred choosed that distribution and so it's the probability that the distribution Θ1 generated E without knowing E already.\
• P(E|Θ₁) is the probability that we get a plain cookie knowing that Fred chose the distribution Θ1 where I get a plain cookie with probability 30/40. So, it is the probability of getting the empirical distribution E knowing the probability distribution Θ₁.\
• P(E) is obatined by summing the joint probabilities of P(Θ₁ ∧ E) and P(Θ₂ ∧ E). It's done so because they are the probability of getting a plain cookie when is chosen Θ₁ or Θ₂ as distribution. They are all the cases that can occur when getting a plain cookie. So, the probability of getting a plain cookie is the sum of those two joint probabilities.


### Describe the different paradigms that can be found within statistical inference (such as"bayesian", "frequentist" [Fisher, Neyman]).

Different schools of statistical inference have become established. These schools, or paradigms, are not mutually exclusive, and methods that work well under one paradigm often have attractive interpretations under other paradigms.

There are mainly two: \
1- ***Frequentist inference*** \
2- ***Bayesian inference***

**Frequentist inference** 

Frequentist statistics is about repeatability and gathering more data. The frequentist interpretation of probability is the long-run frequency of repeatable experiments. For example, saying that the probability of a coin landing heads being 0.5 means that if we were to flip the coin enough times, we would see heads 50% of the time. This probability of 0.5 has been obatined by repeating the experiment of tossing a coin and has been observed that the coin lands head with relative frequency of 0.5. \
With this definition, you can’t really define a probability for events that aren’t repeatable. For instance, before the 2018 FIFA world cup, let’s say that you read somewhere that Brazil’s probability of winning the tournament was 14%. This probability cannot really be explained with a purely frequentist interpretation, because the experiment is not repeatable. The 2018 world cup is only going to happen once, and it’s impossible to repeat the exact experiment with the same exact conditions, players and preceding events. This means that it's impossible to calculate that probability using the frequentist approach.

In frequentist inference, any uncertainty in probabilistic estimates that we derive are considered to be due to sampling error alone which is the differences between the actual population and the sample you ended up drawing. This means that the distribution of the observed variable that we obtained by repeating the experiment isn't describing the empirical distribution of a real population relatively to the same experiment. \
For frequentists, having large samples, solves most problems since your sample becomes closer to the true population distribution. 

**Bayesian inference** \
Bayesian inference is the process of deducing properties about a population using the Bayes’ theorem. The role of this theorem in bayesian inference has been already explained above. \
This paradigm is more general since we don't need repeatable events to apply this approach but we can assign a probability to any event. \
Given an empirical distribution E, I want to obtain the probability distribution from a family of distributions that, most likely, generated the E. So, in this case, I don't do repeatedly samples of the experiment to obtain a probability distribution but I apply the Bayes' theorem to find P(Θ|E) (probability that Θ generated E knowing E). Obtaining the most likely probability distribution means to find the most likely values of the state of nature Θ that describe this distribution.




[1] [https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) \
[2] [https://en.wikipedia.org/wiki/Bayesian_inference](https://en.wikipedia.org/wiki/Bayesian_inference) \
[3] [https://towardsdatascience.com/frequentist-and-bayesian-inference-83af2595f172](https://towardsdatascience.com/frequentist-and-bayesian-inference-83af2595f172)
