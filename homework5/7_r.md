**Made by Nicolò Baroncini (1834907)**

## Research (7_R)
### Explain the Bayes Theorem and its key role in statistical induction.
In the research 4_R we have seen the Bayes theorem explained with relative frequencies. Since relative frequencies are the realization of the theoretical concept of probability, the Bayes theorem works also for probability. \
It says that the conditional probability P(A | B) can be calculated with the following formula:

![image](https://user-images.githubusercontent.com/78324346/138822456-dd78013c-dffc-463d-890a-372d1f861e38.png)

In statistical induction we have an empirical distribution, that has been obtained by observing a phenomenon, and a family of probability distribution. An example of family can be the family of normal distributions. Every member of the family can be obtained by changing, for instance, variance or mean. The parameters that can be set to pick a specific member of the family are contained in a vector called ***state of nature*** and its symbol is Θ. \
If we call E the empirical distribution we have that, in statistical induction, we are interested in knowing P(Θ | E). It is the probability that, knowing E, it was generated by a member of the familty distribution that has as values of the parameters the ones contained in Θ. \
The Bayes theorem comes handy to calculate this probability since:

![Foglio di brutta-6](https://user-images.githubusercontent.com/78324346/138826607-5c2497f8-cac3-4abf-9ee1-2ff3fc659c0b.jpg)

where\
•P(Θ) is the prior probability, it is the probability that Θ is the state of nature that generated E before that E is observed. \
•P(Θ|E) is the posterior probability. As already explained it is the probability that Θ generated E knowing E. \
•P(E|Θ) is called "likelihood of Θ", it is the probability that E is generated by Θ knowing Θ.\
•P(E) is the sum of the joint probability of E with any possible Θ since E can be generated by any of them.

This concept can be easily seen in the next example.

Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?

In this example the Θ is the bowl from which Fred picked a cookie and the E is the cookie picked. We want to know the probability P(Θ|E) which is the probability that the cookie, and so the empirical distribution, has been "generated" by the distribution represented by Θ that is the distribution of a specific bowl. \ \
Let Θ₁ correspond to bowl 1 and Θ₂ to bowl 2. It is given that the bowls are identical from Fred's point of view, thus P(Θ₁) = P(Θ₂) so both are equal to 0.5. The E is the observation that we picked a plain cookie. \
We know that P(E|Θ₁) = 30/40 = 0.75 and P(E|Θ₂) = 20/40 = 0.5.
So I get, using the Bayes theorem, the following value.

![Foglio di brutta-6 2](https://user-images.githubusercontent.com/78324346/138833383-b5e2c8a7-bbd5-4471-ab04-a71df449c50f.jpg)

Where:
• P(Θ₁) is the probability that Fred choosed that distribution and so it's the probability that the distribution Θ1 generated E without knowing E already.\
• P(E|Θ₁) is the probability that we get a plain cookie knowing that Fred chose the distribution Θ1 where I get a plain cookie with probability 30/40. So, it is the probability of getting the empirical distribution E knowing the probability distribution Θ₁.\
• P(E) is obatined by summing the joint probabilities of P(Θ₁ ∧ E) and P(Θ₂ ∧ E). It's done so because they are the probability of getting a plain cookie when is chosen Θ₁ or Θ₂ as distribution. They are all the cases that can occur when getting a plain cookie. So, the probability of getting a plain cookie is the sum of those two joint probabilities.


### Describe the different paradigms that can be found within statistical inference (such as"bayesian", "frequentist" [Fisher, Neyman]).

Different schools of statistical inference have become established. These schools, or paradigms, are not mutually exclusive, and methods that work well under one paradigm often have attractive interpretations under other paradigms.

There are mainly two: \
1- ***Frequentist inference*** \
2- ***Bayesian inference***

**Frequentist inference** 

This paradigm calibrates the plausibility of propositions by considering (notional) repeated sampling of a population distribution to produce datasets similar to the one at hand. By considering the dataset's characteristics under repeated sampling, the frequentist properties of a statistical proposition can be quantified—although in practice this quantification may be challenging.

[1] [https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) \
[2] [https://en.wikipedia.org/wiki/Bayesian_inference](https://en.wikipedia.org/wiki/Bayesian_inference) \
[3] [https://en.wikipedia.org/wiki/Statistical_inference#:~:text=the%20conditional%20mean%2C%20.-,Paradigms%20for%20inference,statistical%20inference%20have%20become%20established.&text=Bandyopadhyay%20%26%20Forster%20describe%20four%20paradigms,Information%20Criterion%2Dbased%20statistics%22.](https://en.wikipedia.org/wiki/Statistical_inference#:~:text=the%20conditional%20mean%2C%20.-,Paradigms%20for%20inference,statistical%20inference%20have%20become%20established.&text=Bandyopadhyay%20%26%20Forster%20describe%20four%20paradigms,Information%20Criterion%2Dbased%20statistics%22.)
