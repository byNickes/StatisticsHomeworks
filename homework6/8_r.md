**Made by Nicolò Baroncini (1834907)**

## Research (8_R)
### Do a research about the law of large numbers LLN and the various definitions of convergence.
Before explaining the Law of Large Numbers we need to know what the convergence of random variables is.

**Convergence in distribution**\
Convergence in distribution is in some sense the weakest type of convergence. All it says is that the CDF of the sequence of random variables Xₙ converges to the CDF of X as n goes to infinity. It does not require any dependence between the sequence Xₙ and X.

Here is a formal definition of convergence in distribution:

![image](https://user-images.githubusercontent.com/78324346/139832899-33768556-5ad0-4ae1-a2f5-40ef7afc708d.png)

**Convergence in probability**\
Convergence in probability is stronger than convergence in distribution. In particular, for a sequence X₁, X₂, X₃,... to converge to a random variable X, we must have that P(|Xₙ−X|≥ϵ) goes to 0 as n→∞, for any ϵ>0.

Here is the formal definition of convergence in probability:

![image](https://user-images.githubusercontent.com/78324346/139833313-336855aa-1286-4e26-ade1-f6ad995da4d4.png)

**Almost sure convergence**\
Almost sure convergence is the strongest type of convergence.

The definition is as following:\
To say that the sequence Xₙ converges almost surely or almost everywhere or with probability 1 or strongly towards X means that

![image](https://user-images.githubusercontent.com/78324346/139833897-8c3ecebd-3f1d-49a2-b3ee-e93e2ad5370c.png)

This means that the values of Xₙ approach the value of X, in the sense that events for which Xₙ does not converge to X have probability 0. \
Almost sure convergence is often denoted by adding the letters a.s. over an arrow indicating convergence as shown below:

![image](https://user-images.githubusercontent.com/78324346/139834045-6d7511a0-7355-4a30-954e-79156152511e.png)

Notice that the order of strength of the definitions is the follow:

convergence in distribution <= convergence in probability <= convergence almost surely

It's possible now to talk about the law of large numbers. \
The Law of Large Numbers describes the behavior of the mean of a sequence of n trials of a random variable, which are independent on each other and following the same distribution of probability, as n tends to infinity. 
In few words, we expect to see that if the expected value of a random variable is μ, then the mean of infinite trials of that random variable is μ.
We have 2 forms of this law. \
Before talking about the definition of the two forms let's define:

- Sample average as

![image](https://user-images.githubusercontent.com/78324346/139836034-b98974c9-ed82-41e2-8ea2-8d20ffe5f0cb.png)

- Sequence of random variables Xₙ as an X₁, X₂, X₃, ... infinite sequence of independent and identically distributed Lebesgue integrable random variables with expected value E(X₁) = E(X₂) = ...= µ.

**Weak law of large numbers** \
The weak law of large numbers states that the sample average converges in probability towards the expected value 

![image](https://user-images.githubusercontent.com/78324346/139835599-891e5dec-21ca-4789-b2ca-a1283464c18d.png)

That is, for any positive number ε.

![image](https://user-images.githubusercontent.com/78324346/139835646-bf64dbf9-045d-4367-b6ac-29bccf4cf66d.png)

Interpreting this result, the weak law states that for any nonzero margin specified (ε), no matter how small, with a sufficiently large sample there will be a very high probability that the average of the observations will be close to the expected value.

**Strong law of large numbers** \
The strong law of large numbers states that the sample average converges almost surely to the expected value

![image](https://user-images.githubusercontent.com/78324346/139836264-712a9198-1e49-4cc3-b26d-949c1785337a.png)

That is

![image](https://user-images.githubusercontent.com/78324346/139836289-e737678f-8a9e-4718-b61a-2ff6db2d47ba.png)

What this means is that the probability that, as the number of trials n goes to infinity, the average of the observations converges to the expected value, is equal to one. \
Almost sure convergence is also called strong convergence of random variables. This version is called the strong law because random variables which converge strongly (almost surely) are guaranteed to converge weakly (in probability).

### Do a research about the convergence of the Binomial to the Normal and Poisson distributions.
First of all, the Binomial distribution is a discrete probability distribution that describes the number of successes within a Bernoulli process. So, a Binomial random variable is the random variable Sₙ = X₁ + X₂ + … + Xₙ who sums n independent random variable of the same Bernoulli distribution.

The parameters of the binomial distribution B(n, p) are: \
– **n**: the number of trials; \
– **p**: the probability of success of the single Bernoulli trial. \
Let be q = 1 – p, then the probability distribution is:

![image](https://user-images.githubusercontent.com/78324346/139873946-8f5a414c-c4e9-493c-9331-ff6bd457c2b1.png)

If n is large enough a reasonable approximation to B(n, p) is given by the normal distribution with the following parameters

![image](https://user-images.githubusercontent.com/78324346/139875513-c40606bf-73cf-4e90-a750-b162517d0849.png)


###  Do a research about the central limit theorem.
In probability theory, the central limit theorem establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. \
So this theorem says that as we take more samples the graph of the sample mean will look more and more like a normal distribution. \
For example, suppose that a sample is obtained containing many observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the probability distribution of the average will closely approximate a normal distribution. A simple example of this is that if one flips a coin many times, the probability of getting a given number of heads will approach a normal distribution, with the mean equal to half the total number of flips. At the limit of an infinite number of flips, it will equal a normal distribution.


[1][https://www.probabilitycourse.com/chapter7/7_2_4_convergence_in_distribution.php](https://www.probabilitycourse.com/chapter7/7_2_4_convergence_in_distribution.php) \
[2][https://www.probabilitycourse.com/chapter7/7_2_5_convergence_in_probability.php](https://www.probabilitycourse.com/chapter7/7_2_5_convergence_in_probability.php) \
[3][https://en.wikipedia.org/wiki/Convergence_of_random_variables](https://en.wikipedia.org/wiki/Convergence_of_random_variables) \
[4][https://en.wikipedia.org/wiki/Law_of_large_numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) \
[5][https://en.wikipedia.org/wiki/Binomial_distribution](https://en.wikipedia.org/wiki/Binomial_distribution) \
[6][https://en.wikipedia.org/wiki/Central_limit_theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)
